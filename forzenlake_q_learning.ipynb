{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm==4.66.5 (from -r requirements.txt (line 1))\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting matplotlib==3.7.5 (from -r requirements.txt (line 2))\n",
      "  Using cached matplotlib-3.7.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.7 kB)\n",
      "Collecting pandas==1.5.3 (from -r requirements.txt (line 3))\n",
      "  Using cached pandas-1.5.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting gymnasium==0.29.1 (from -r requirements.txt (line 4))\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy==1.22.0 (from -r requirements.txt (line 5))\n",
      "  Using cached numpy-1.22.0.zip (11.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pygame==2.5.2 (from -r requirements.txt (line 6))\n",
      "  Using cached pygame-2.5.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/shellyli/anaconda3/lib/python3.11/site-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/shellyli/anaconda3/lib/python3.11/site-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/shellyli/anaconda3/lib/python3.11/site-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/shellyli/anaconda3/lib/python3.11/site-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shellyli/anaconda3/lib/python3.11/site-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/shellyli/anaconda3/lib/python3.11/site-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/shellyli/anaconda3/lib/python3.11/site-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/shellyli/anaconda3/lib/python3.11/site-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/shellyli/anaconda3/lib/python3.11/site-packages (from pandas==1.5.3->-r requirements.txt (line 3)) (2024.1)\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Cannot install -r requirements.txt (line 2), -r requirements.txt (line 3) and numpy==1.22.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    The user requested numpy==1.22.0\n",
      "    matplotlib 3.7.5 depends on numpy<2 and >=1.20\n",
      "    pandas 1.5.3 depends on numpy>=1.21.0; python_version >= \"3.10\"\n",
      "    pandas 1.5.3 depends on numpy>=1.23.2; python_version >= \"3.11\"\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from agent.base_agent import BaseAgent\n",
    "from utils import seed_everything\n",
    "\n",
    "\n",
    "# Q-Learning Agent Class inheriting from Agent\n",
    "class QLearningAgent(BaseAgent):\n",
    "    def __init__(self, env, hyperparams):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent class for interacting with the environment and learning\n",
    "        \"\"\"\n",
    "        super(QLearningAgent, self).__init__(env, hyperparams)\n",
    "        self.Q_table = np.zeros((self.state_dim, self.num_actions))\n",
    "        \n",
    "        # Additional hyperparameters\n",
    "        self.learning_rate = hyperparams['learning_rate']\n",
    "        self.discount = hyperparams['discount_factor']\n",
    "        self.epsilon = hyperparams['epsilon_max']\n",
    "        self.epsilon_min = hyperparams['epsilon_min']\n",
    "        self.epsilon_decay = hyperparams['epsilon_decay']\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Learn from a single experience using the Q-learning update rule\n",
    "        \"\"\"\n",
    "        best_next_action = np.argmax(self.Q_table[next_state])\n",
    "        td_target = reward + self.discount * self.Q_table[next_state, best_next_action] * (1 - done)\n",
    "        td_error = td_target - self.Q_table[state, action]\n",
    "        self.Q_table[state, action] += self.learning_rate * td_error\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        \"\"\"\n",
    "        Update epsilon value for epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the trained Q-table\n",
    "        \"\"\"\n",
    "        np.save(path, self.Q_table)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load the Q-table\n",
    "        \"\"\"\n",
    "        self.Q_table = np.load(path)\n",
    "\n",
    "# Trainer Class\n",
    "class Trainer:\n",
    "    def __init__(self, env, agent, hyperparams):\n",
    "        \"\"\"\n",
    "        Trainer class to manage training and evaluation of the agent\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.hyperparams = hyperparams\n",
    "        self.reward_history = []\n",
    "        self.step_history = []\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent for a specified number of episodes\n",
    "        \"\"\"\n",
    "        total_steps = 0\n",
    "        for episode in range(1, self.hyperparams['max_episodes'] + 1):\n",
    "            state, _ = self.env.reset(seed=seed)  # Remove fixed seed\n",
    "            done, truncation = False, False\n",
    "            step_size, episode_reward = 0, 0\n",
    "\n",
    "            while not done and not truncation:\n",
    "                # Select and execute an action\n",
    "                action = self.agent.select_action(state)\n",
    "                next_state, reward, done, truncation, _ = self.env.step(action)\n",
    "\n",
    "                # Learn from the experience\n",
    "                self.agent.learn(state, action, reward, next_state, done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step_size += 1\n",
    "\n",
    "            self.reward_history.append(episode_reward)\n",
    "            self.step_history.append(step_size)  # Track steps for each episode\n",
    "            total_steps += step_size\n",
    "            self.agent.update_epsilon()\n",
    "\n",
    "            # Save the model at specified intervals\n",
    "            if episode % self.hyperparams['save_interval'] == 0:\n",
    "                os.makedirs(self.hyperparams['model_dir'], exist_ok=True)\n",
    "                self.agent.save(os.path.join(self.hyperparams['model_dir'], f'{episode}.npy'))\n",
    "                print('\\n~~~~~~Interval Save: Model saved.\\n')\n",
    "\n",
    "            # Print episode information at specified intervals\n",
    "            if episode % self.hyperparams['print_interval'] == 0:\n",
    "                print(f\"Episode: {episode}, Total Steps: {total_steps}, Ep Step: {step_size}, Raw Reward: {episode_reward:.2f}, Epsilon: {self.agent.epsilon:.2f}\")\n",
    "        \n",
    "        # Plot results after training\n",
    "        self.plot_training(episode)\n",
    "\n",
    "    def test(self, max_episodes=None):\n",
    "        \"\"\"\n",
    "        Test the trained agent for a specified number of episodes\n",
    "        \"\"\"\n",
    "        # Load the trained Q-table\n",
    "        self.agent.load(os.path.join(self.hyperparams['model_dir'], f\"{self.hyperparams['train_episodes']}.npy\"))\n",
    "        # Set epsilon to 0 to always exploit\n",
    "        self.agent.epsilon = 0.0\n",
    "\n",
    "        if max_episodes is None:\n",
    "            max_episodes = self.hyperparams['max_episodes']\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            state, _ = self.env.reset(seed=seed)\n",
    "            done, truncation = False, False\n",
    "            step_size, episode_reward = 0, 0\n",
    "\n",
    "            while not done and not truncation:\n",
    "                # Render the environment if desired\n",
    "                if self.hyperparams['render']:\n",
    "                    self.env.render()\n",
    "\n",
    "                # Select an action\n",
    "                action = self.agent.select_action(state)\n",
    "                next_state, reward, done, truncation, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step_size += 1\n",
    "\n",
    "            # Print episode information\n",
    "            print(f\"Episode: {episode}, Steps: {step_size}, Reward: {episode_reward:.2f}\")\n",
    "\n",
    "    def plot_training(self, episode):\n",
    "        \"\"\"\n",
    "        Plot the training reward history and step convergence\n",
    "        \"\"\"\n",
    "        # Plot Rewards\n",
    "        sma_window = 50  # Simple Moving Average window\n",
    "        if len(self.reward_history) >= sma_window:\n",
    "            sma = np.convolve(self.reward_history, np.ones(sma_window)/sma_window, mode='valid')\n",
    "        else:\n",
    "            sma = self.reward_history\n",
    "        plt.figure()\n",
    "        plt.title(\"Rewards\")\n",
    "        plt.plot(self.reward_history, label='Raw Reward', color='#F6CE3B', alpha=0.6)\n",
    "        plt.plot(range(len(sma)), sma, label=f'SMA {sma_window}', color='#385DAA')\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.legend()\n",
    "        if episode == self.hyperparams['max_episodes']:\n",
    "            os.makedirs(self.hyperparams['plot_dir'], exist_ok=True)\n",
    "            plt.savefig(os.path.join(self.hyperparams['plot_dir'], 'reward_plot.png'), format='png', dpi=600, bbox_inches='tight')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "        # Plot Steps Convergence\n",
    "        if len(self.step_history) >= sma_window:\n",
    "            sma_steps = np.convolve(self.step_history, np.ones(sma_window)/sma_window, mode='valid')\n",
    "        else:\n",
    "            sma_steps = self.step_history\n",
    "        plt.figure()\n",
    "        plt.title(\"Steps Convergence\")\n",
    "        plt.plot(self.step_history, label='Raw Steps', color='#4CAF50', alpha=0.6)\n",
    "        plt.plot(range(len(sma_steps)), sma_steps, label=f'SMA {sma_window}', color='#FF5733')\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Steps\")\n",
    "        plt.legend()\n",
    "        if episode == self.hyperparams['max_episodes']:\n",
    "            plt.savefig(os.path.join(self.hyperparams['plot_dir'], 'steps_plot.png'), format='png', dpi=600, bbox_inches='tight')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "# Main Function to Set Up and Train/Test the Model\n",
    "if __name__ == \"__main__\":\n",
    "    method_name = \"q_learning\"\n",
    "    train_mode = True\n",
    "\n",
    "    train_episodes = 50000\n",
    "    render = not train_mode\n",
    "    map_size = 4  # Start with 4x4 map\n",
    "    seed = 0\n",
    "\n",
    "    # Config paths and hyperparameters\n",
    "    config = {\n",
    "        \"train_mode\": train_mode,\n",
    "        \"model_dir\": f'output/{method_name}/model/{map_size}x{map_size}/',\n",
    "        \"plot_dir\": f'output/{method_name}/plt/{map_size}x{map_size}/', \n",
    "        \"save_interval\": 5000,\n",
    "        \"print_interval\": 500,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"max_episodes\": train_episodes if train_mode else 10,\n",
    "        \"train_episodes\": train_episodes,\n",
    "        \"max_steps\": 1000,\n",
    "        \"render\": render,\n",
    "        \"epsilon_max\": 1.0 if train_mode else -1,\n",
    "        \"epsilon_min\": 0.1,\n",
    "        \"epsilon_decay\": 0.9999,\n",
    "        \"map_size\": map_size,\n",
    "        \"render_fps\": 6,\n",
    "    }\n",
    "\n",
    "    # Set seed\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # Create environment\n",
    "    env = gym.make(\n",
    "        'FrozenLake-v1',\n",
    "        map_name=f\"{config['map_size']}x{config['map_size']}\",\n",
    "        is_slippery=True,\n",
    "        max_episode_steps=config['max_steps'],\n",
    "        render_mode=\"human\" if config['render'] else None\n",
    "    )\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.metadata['render_fps'] = config['render_fps']\n",
    "\n",
    "    # Create agent and trainer\n",
    "    agent = QLearningAgent(env, config)\n",
    "    trainer = Trainer(env, agent, config)\n",
    "\n",
    "    # Train or test\n",
    "    if config['train_mode']:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        trainer.test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
