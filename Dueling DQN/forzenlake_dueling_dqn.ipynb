{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tqdm==4.66.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.66.5)\n",
      "Collecting matplotlib==3.7.5 (from -r requirements.txt (line 2))\n",
      "  Downloading matplotlib-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting pandas==1.5.3 (from -r requirements.txt (line 3))\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting gymnasium==0.29.1 (from -r requirements.txt (line 4))\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy==1.22.0 (from -r requirements.txt (line 5))\n",
      "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting pygame==2.5.2 (from -r requirements.txt (line 6))\n",
      "  Downloading pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.5->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 3)) (2023.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->-r requirements.txt (line 4)) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium==0.29.1->-r requirements.txt (line 4))\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.5->-r requirements.txt (line 2)) (1.16.0)\n",
      "Downloading matplotlib-3.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m657.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, pygame, numpy, pandas, gymnasium, matplotlib\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.9.2\n",
      "    Uninstalling matplotlib-3.9.2:\n",
      "      Successfully uninstalled matplotlib-3.9.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "cudf 24.6.0 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "cugraph 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "cugraph-dgl 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "cugraph-pyg 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "cugraph-service-server 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "dask-cuda 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "dask-cudf 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "dask-cudf 24.6.0 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "dask-expr 1.1.1 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
      "kvikio 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "librosa 0.10.1 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.22.0 which is incompatible.\n",
      "nx-cugraph 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "pylibraft 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "raft-dask 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "rmm 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "scipy 1.14.0 requires numpy<2.3,>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
      "ucx-py 0.38.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "ucxx 0.38.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "zarr 2.18.2 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
      "lightning-thunder 0.2.0.dev0 requires numpy<2,>=1.23.0, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed farama-notifications-0.0.4 gymnasium-0.29.1 matplotlib-3.7.5 numpy-1.22.0 pandas-1.5.3 pygame-2.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from agent.base_agent import BaseAgent\n",
    "from utils import seed_everything\n",
    "\n",
    "# Set device to GPU if available\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "# Environment setup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Set CUDA launch blocking to 1 for debugging CUDA-related errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer Class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Experience replay buffer used to store transitions\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done):\n",
    "        \"\"\"\n",
    "        Store a transition in the buffer\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, next_state, reward, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of transitions from the buffer\n",
    "        \"\"\"\n",
    "        indices = np.random.choice(len(self.buffer), size=batch_size, replace=False)\n",
    "        states, actions, next_states, rewards, dones = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "        # Convert sampled transitions to tensors\n",
    "        states = torch.stack(states).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool, device=device)\n",
    "\n",
    "        return states, actions, next_states, rewards, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two-stream Q-net class\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self, num_actions, input_dim):\n",
    "        \"\"\"\n",
    "        Neural network for Deep Q-Learning\n",
    "        \"\"\"\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # Value stream\n",
    "        self.value = nn.Linear(32, 1)\n",
    "        # Advantage stream\n",
    "        self.advantage = nn.Linear(32, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layers(x)\n",
    "        value = self.value(x)\n",
    "        advantage = self.advantage(x)\n",
    "        # Compute Q-values: Q(s, a) = V(s) + A(s, a) - mean(A(s, a))\n",
    "        q_values = value + advantage - advantage.mean()\n",
    "        return q_values\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for linear layers using Kaiming uniform initialization\n",
    "        \"\"\"\n",
    "        for layer in self.fc_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent Class inheriting from Agent\n",
    "class DQNAgent(BaseAgent):\n",
    "    def __init__(self, env, hyperparams):\n",
    "        \"\"\"\n",
    "        DQN Agent class for interacting with the environment and learning\n",
    "        \"\"\"\n",
    "        super(DQNAgent, self).__init__(env, hyperparams)\n",
    "        self.replay_memory = ReplayBuffer(hyperparams['memory_capacity'])\n",
    "\n",
    "        # Main and target networks\n",
    "        self.model = Qnet(self.num_actions, self.state_dim).to(device)\n",
    "        self.target_network = Qnet(self.num_actions, self.state_dim).to(device)\n",
    "        self.update_target_network()\n",
    "\n",
    "        # Optimizer and loss function\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=hyperparams['learning_rate'])\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Additional hyperparameters\n",
    "        self.discount = hyperparams['discount_factor']\n",
    "        self.epsilon = hyperparams['epsilon_max']\n",
    "        self.epsilon_min = hyperparams['epsilon_min']\n",
    "        self.epsilon_decay = hyperparams['epsilon_decay']\n",
    "        self.clip_grad_norm = hyperparams['clip_grad_norm']\n",
    "        self.loss_history = []\n",
    "        self.running_loss = 0\n",
    "        self.learned_counts = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        with torch.no_grad():\n",
    "            Q_values = self.model(state)\n",
    "            return torch.argmax(Q_values).item()\n",
    "\n",
    "    def learn(self, batch_size, done):\n",
    "        \"\"\"\n",
    "        Learn from a batch of experiences using the DQN update rule\n",
    "        \"\"\"\n",
    "        # Sample a batch from the replay buffer\n",
    "        states, actions, next_states, rewards, dones = self.replay_memory.sample(batch_size)\n",
    "        actions = actions.unsqueeze(1)\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        dones = dones.unsqueeze(1)\n",
    "        \n",
    "        # Compute predicted Q-values and target Q-values\n",
    "        predicted_q = self.model(states).gather(dim=1, index=actions)\n",
    "        with torch.no_grad():\n",
    "            next_target_q_value = self.target_network(next_states).max(dim=1, keepdim=True)[0]\n",
    "        next_target_q_value[dones] = 0\n",
    "        y_js = rewards + (self.discount * next_target_q_value)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.criterion(predicted_q, y_js)\n",
    "        self.running_loss += loss.item()\n",
    "        self.learned_counts += 1\n",
    "        if done:\n",
    "            self.loss_history.append(self.running_loss / self.learned_counts)\n",
    "            self.running_loss, self.learned_counts = 0, 0\n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad_norm)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Update the target network by copying weights from the main network\n",
    "        \"\"\"\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        \"\"\"\n",
    "        Update epsilon value for epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the trained model\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load the trained model\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Class\n",
    "class Trainer:\n",
    "    def __init__(self, env, agent, hyperparams):\n",
    "        \"\"\"\n",
    "        Trainer class to manage training and evaluation of the agent\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.hyperparams = hyperparams\n",
    "        self.reward_history = []\n",
    "        self.step_history = []\n",
    "        if hasattr(agent, 'replay_memory'):\n",
    "            self.use_replay_memory = True\n",
    "        else:\n",
    "            self.use_replay_memory = False\n",
    "\n",
    "    def state_preprocess(self, state):\n",
    "        \"\"\"\n",
    "        Convert the state to one-hot representation \n",
    "        \"\"\"\n",
    "        onehot_vector = torch.zeros(self.env.observation_space.n, dtype=torch.float32, device=device)\n",
    "        onehot_vector[state] = 1\n",
    "        return onehot_vector\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent for a specified number of episodes\n",
    "        \"\"\"\n",
    "        total_steps = 0\n",
    "        for episode in range(1, self.hyperparams['max_episodes'] + 1):\n",
    "            state, _ = self.env.reset(seed=seed)\n",
    "            state = self.state_preprocess(state)\n",
    "            done, truncation = False, False\n",
    "            step_size, episode_reward = 0, 0\n",
    "\n",
    "            while not done and not truncation:\n",
    "                # Select and execute an action\n",
    "                action = self.agent.select_action(state)\n",
    "                next_state, reward, done, truncation, _ = self.env.step(action)\n",
    "                next_state = self.state_preprocess(next_state)\n",
    "\n",
    "                # use ReplayBuffer\n",
    "                if self.use_replay_memory:\n",
    "                    self.agent.replay_memory.store(state, action, next_state, reward, done)\n",
    "                    if len(self.agent.replay_memory) > self.hyperparams['batch_size'] and sum(self.reward_history) > 0:\n",
    "                        self.agent.learn(self.hyperparams['batch_size'], (done or truncation))\n",
    "                        # Update target network periodically\n",
    "                        if total_steps % self.hyperparams['update_frequency'] == 0:\n",
    "                            self.agent.update_target_network()\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step_size += 1\n",
    "\n",
    "            self.reward_history.append(episode_reward)\n",
    "            self.step_history.append(step_size)  # Track steps for each episode\n",
    "            total_steps += step_size\n",
    "            self.agent.update_epsilon()\n",
    "\n",
    "            # Save the model at specified intervals\n",
    "            if episode % self.hyperparams['save_interval'] == 0:\n",
    "                os.makedirs(self.hyperparams['model_dir'], exist_ok=True)\n",
    "                self.agent.save(os.path.join(self.hyperparams['model_dir'], f'{episode}.pth'))\n",
    "                print('\\n~~~~~~Interval Save: Model saved.\\n')\n",
    "            # Print episode information at specified intervals\n",
    "            if episode % self.hyperparams['print_interval'] == 0:\n",
    "                print(f\"Episode: {episode}, Total Steps: {total_steps}, Ep Step: {step_size}, Raw Reward: {episode_reward:.2f}, Epsilon: {self.agent.epsilon:.2f}\")\n",
    "        self.plot_training(episode)\n",
    "\n",
    "    def test(self, max_episodes=None):\n",
    "        \"\"\"\n",
    "        Test the trained agent\n",
    "        \"\"\"\n",
    "        # Load the trained model\n",
    "        self.agent.load(os.path.join(self.hyperparams['model_dir'], f\"{self.hyperparams['train_episodes']}.pth\"))\n",
    "        self.agent.model.eval()\n",
    "\n",
    "        if max_episodes is None:\n",
    "            max_episodes = self.hyperparams['max_episodes']\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            state, _ = self.env.reset(seed=seed)\n",
    "            done, truncation = False, False\n",
    "            step_size, episode_reward = 0, 0\n",
    "\n",
    "            while not done and not truncation:\n",
    "                # Select an action\n",
    "                state = self.state_preprocess(state)\n",
    "                action = self.agent.select_action(state)\n",
    "                next_state, reward, done, truncation, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                step_size += 1\n",
    "\n",
    "            # Print episode information\n",
    "            print(f\"Episode: {episode}, Steps: {step_size}, Reward: {episode_reward:.2f}\")\n",
    "\n",
    "\n",
    "    def plot_training(self, episode):\n",
    "        \"\"\"\n",
    "        Plot the training reward history, loss history, and step convergence\n",
    "        \"\"\"\n",
    "        # Plot Rewards\n",
    "        sma = np.convolve(self.reward_history, np.ones(50)/50, mode='valid')\n",
    "        plt.figure()\n",
    "        plt.title(\"Rewards\")\n",
    "        plt.plot(self.reward_history, label='Raw Reward', color='#F6CE3B', alpha=1)\n",
    "        plt.plot(sma, label='SMA 50', color='#385DAA')\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.legend()\n",
    "        if episode == self.hyperparams['max_episodes']:\n",
    "            os.makedirs(self.hyperparams['plot_dir'], exist_ok=True)\n",
    "            plt.savefig(os.path.join(self.hyperparams['plot_dir'], 'reward_plot.png'), format='png', dpi=600, bbox_inches='tight')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "        # Plot Loss\n",
    "        plt.figure()\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(self.agent.loss_history, label='Loss', color='#CB291A', alpha=1)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        if episode == self.hyperparams['max_episodes']:\n",
    "            plt.savefig(os.path.join(self.hyperparams['plot_dir'], 'loss_plot.png'), format='png', dpi=600, bbox_inches='tight')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "        # Plot Steps Convergence\n",
    "        sma_steps = np.convolve(self.step_history, np.ones(50)/50, mode='valid')\n",
    "        plt.figure()\n",
    "        plt.title(\"Steps Convergence\")\n",
    "        plt.plot(self.step_history, label='Raw Steps', color='#4CAF50', alpha=1)\n",
    "        plt.plot(sma_steps, label='SMA 50', color='#FF5733')\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Steps\")\n",
    "        plt.legend()\n",
    "        if episode == self.hyperparams['max_episodes']:\n",
    "            plt.savefig(os.path.join(self.hyperparams['plot_dir'], 'steps_plot.png'), format='png', dpi=600, bbox_inches='tight')\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first tested with hyperparameters set to the default value of traditional DQN, and we evaluated its performance with the avergae reward of the last 100 traning episodes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main Function to Set Up and Train/Test the Model\n",
    "if __name__ == \"__main__\":\n",
    "    method_name = \"dueling_dqn\"\n",
    "    train_mode = True\n",
    "\n",
    "    train_episodes = 10000 \n",
    "    render = not train_mode\n",
    "    map_size = 4 # Start with 4x4 map\n",
    "    seed = 1\n",
    "\n",
    "    # build config\n",
    "    config = {\n",
    "        \"train_mode\": train_mode,\n",
    "        \"method_name\": method_name,\n",
    "        \"model_dir\": f'output/{method_name}/model/{map_size}x{map_size}/',\n",
    "        \"plot_dir\": f'output/{method_name}/plt/{map_size}x{map_size}/',\n",
    "        \"save_interval\": 500,\n",
    "        \"print_interval\": 100,\n",
    "        \"clip_grad_norm\": 3,\n",
    "        \"learning_rate\": 6e-4 , \n",
    "        \"discount_factor\": 0.93,  \n",
    "        \"batch_size\": 32,\n",
    "        \"update_frequency\": 10,\n",
    "        \"max_episodes\": train_episodes if train_mode else 10,\n",
    "        \"train_episodes\": train_episodes,\n",
    "        \"max_steps\": 200,\n",
    "        \"render\": render,\n",
    "        \"epsilon_max\": 0.9 if train_mode else -1, #0.999\n",
    "        \"epsilon_min\": 0.01,\n",
    "        \"epsilon_decay\": 0.999,\n",
    "        \"memory_capacity\": 4000 if train_mode else 0,\n",
    "        \"map_size\": map_size,\n",
    "        \"render_fps\": 6,\n",
    "    }\n",
    "\n",
    "    # set seed\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # Create environment\n",
    "    env = gym.make(\n",
    "        'FrozenLake-v1', \n",
    "        map_name=f\"{config['map_size']}x{config['map_size']}\", \n",
    "        is_slippery=True, \n",
    "        max_episode_steps=config['max_steps'], \n",
    "        render_mode=\"human\" if config['render'] else None)\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.metadata['render_fps'] = config['render_fps']\n",
    "\n",
    "    # Create agent and trainer\n",
    "    agent = DQNAgent(env, config)\n",
    "    trainer = Trainer(env, agent, config)\n",
    "\n",
    "    # Train or test\n",
    "    if config['train_mode']:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        trainer.test()\n",
    "    avg_reward = np.mean(trainer.reward_history[-100:])\n",
    "    print(avg_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter values after the \"#\" are default values of traditional DQN，here we modify them for better performance.\n",
    "Here they are set to the combination with best preformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main Function to Set Up and Train/Test the Model\n",
    "if __name__ == \"__main__\":\n",
    "    method_name = \"dueling_dqn\"\n",
    "    train_mode = True\n",
    "\n",
    "    train_episodes = 10000 \n",
    "    render = not train_mode\n",
    "    map_size = 4 # Start with 4x4 map\n",
    "    seed = 1\n",
    "\n",
    "    # build config\n",
    "    config = {\n",
    "        \"train_mode\": train_mode,\n",
    "        \"method_name\": method_name,\n",
    "        \"model_dir\": f'output/{method_name}/model/{map_size}x{map_size}/',\n",
    "        \"plot_dir\": f'output/{method_name}/plt/{map_size}x{map_size}/',\n",
    "        \"save_interval\": 500,\n",
    "        \"print_interval\": 100,\n",
    "        \"clip_grad_norm\": 3,\n",
    "        \"learning_rate\": 5e-4 , #6e-4 \n",
    "        \"discount_factor\": 0.9,  #0.93\n",
    "        \"batch_size\": 32,\n",
    "        \"update_frequency\": 10,\n",
    "        \"max_episodes\": train_episodes if train_mode else 10,\n",
    "        \"train_episodes\": train_episodes,\n",
    "        \"max_steps\": 200,\n",
    "        \"render\": render,\n",
    "        \"epsilon_max\": 0.9 if train_mode else -1, #0.999\n",
    "        \"epsilon_min\": 0.01,\n",
    "        \"epsilon_decay\": 0.999,\n",
    "        \"memory_capacity\": 4000 if train_mode else 0,\n",
    "        \"map_size\": map_size,\n",
    "        \"render_fps\": 6,\n",
    "    }\n",
    "\n",
    "    # set seed\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # Create environment\n",
    "    env = gym.make(\n",
    "        'FrozenLake-v1', \n",
    "        map_name=f\"{config['map_size']}x{config['map_size']}\", \n",
    "        is_slippery=True, \n",
    "        max_episode_steps=config['max_steps'], \n",
    "        render_mode=\"human\" if config['render'] else None)\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.metadata['render_fps'] = config['render_fps']\n",
    "\n",
    "    # Create agent and trainer\n",
    "    agent = DQNAgent(env, config)\n",
    "    trainer = Trainer(env, agent, config)\n",
    "\n",
    "    # Train or test\n",
    "    if config['train_mode']:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        trainer.test()\n",
    "    avg_reward = np.mean(trainer.reward_history[-100:])\n",
    "    print(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
